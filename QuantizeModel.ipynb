{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrPL2KJd2fTH"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Xilinx/Vitis-AI.git\n",
        "%cd Vitis-AI/src/vai_quantizer/vai_q_pytorch/pytorch_binding/\n",
        "%cd /content/Vitis-AI/src/vai_quantizer/vai_q_onnx\n",
        "\n",
        "\n",
        "!sh build.sh\n",
        "!pip install pkgs/*.whl\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZavVBmVZGkfl"
      },
      "outputs": [],
      "source": [
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBM-FRvhHXGT"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install xir # Replace with the appropriate version if needed\n",
        "!pip install dill\n",
        "!pip install Ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ir66EwWBSI0L"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
        "os.environ['TORCH_CUDA_ARCH_LIST'] = '7.5'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsnHZZPfH5p7"
      },
      "outputs": [],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQmyzG1Rjs0y"
      },
      "outputs": [],
      "source": [
        "!pip install onnxruntime==1.14.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJyODMTZzuyE"
      },
      "outputs": [],
      "source": [
        "import vai_q_onnx\n",
        "from vai_q_onnx import CalibrationMethod, VitisQuantFormat, QuantType\n",
        "import numpy as np\n",
        "\n",
        "# Custom Calibration Data Reader\n",
        "class CalibrationDataReader:\n",
        "    def __init__(self, calibration_data):\n",
        "        self.calibration_data = calibration_data\n",
        "        self.data_index = 0\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.data_index < len(self.calibration_data):\n",
        "            input_data = self.calibration_data[self.data_index]\n",
        "            self.data_index += 1\n",
        "            return {\"images\": input_data}  # Using 'images' as input node name\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "# Perform Static Quantization\n",
        "def quantize_onnx_model_static(model_input_path, model_output_path, calibration_data):\n",
        "    # Create the calibration data reader\n",
        "    calibration_data_reader = CalibrationDataReader(calibration_data)\n",
        "\n",
        "    # Call the quantize_static API for Vitis AI ONNX quantization with QDQ format\n",
        "    vai_q_onnx.quantize_static(\n",
        "        model_input=model_input_path,                          # Path to the ONNX model\n",
        "        model_output=model_output_path,                        # Output path for quantized model\n",
        "        calibration_data_reader=calibration_data_reader,       # Reader that provides calibration data\n",
        "        quant_format=VitisQuantFormat.QDQ,                     # QDQ quantization format (recommended for x64 performance)\n",
        "        calibrate_method=CalibrationMethod.MinMax,             # Calibration method (MinMax)\n",
        "        input_nodes=[\"images\"],                                # Input node (consistent with ONNX export)\n",
        "        output_nodes=[\"output\"],                               # Output node\n",
        "        op_types_to_quantize=None,                             # Optional: Ops to quantize (None = all ops)\n",
        "        per_channel=False,                                     # Whether to apply per-channel quantization\n",
        "        reduce_range=False,                                    # Optional: Reduces the quantization range\n",
        "        activation_type=QuantType.QInt8,                       # Activation quantization type (int8)\n",
        "        weight_type=QuantType.QInt8,                           # Weight quantization type (int8)\n",
        "        optimize_model=True,                                   # Whether to optimize the model post-quantization\n",
        "        use_external_data_format=False,                        # External data format (optional)\n",
        "        extra_options=None                                     # Extra options (if any)\n",
        "    )\n",
        "\n",
        "    print(f\"Quantized model saved at: {model_output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Define paths for the input and output ONNX models\n",
        "    model_input_path = \"/content/yolov9.onnx\"  # Replace with your ONNX model path\n",
        "    model_output_path = \"/content/quantized_model.onnx\"  # Path to save the quantized model\n",
        "\n",
        "    # Example calibration data (dummy data for illustration purposes)\n",
        "    # Pre-processing: Normalizing the data (you can add more pre-processing if needed)\n",
        "    calibration_data = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
        "    calibration_data = [calibration_data / 255.]\n",
        "\n",
        "    # Perform static quantization\n",
        "    quantize_onnx_model_static(model_input_path, model_output_path, calibration_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XlghQAdRj2mQ"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/Xilinx/pyxir.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXrVKi1gnCK8"
      },
      "outputs": [],
      "source": [
        "!pip install netron\n",
        "import netron\n",
        "netron.start(\"/content/quantized_model_last.onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSKrzD2Mj2sg"
      },
      "outputs": [],
      "source": [
        "model = onnx.load(\"/content/yolov9.onnx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bgoDIl4j2yO"
      },
      "outputs": [],
      "source": [
        "for node in model.graph.node:\n",
        "    print(f\"Node name: {node.name}, op_type: {node.op_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "me1npDRdJZgf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.quantization\n",
        "from pytorch_nndct.apis import torch_quantizer, dump_xmodel\n",
        "import pytorch_nndct as py_nndct\n",
        "\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "%cd yoloModel/\n",
        "\n",
        "\n",
        "def quantize_torch_model(model_path, output_dir):\n",
        "\n",
        "\n",
        "    # Assuming your model and quantizer are already initialized...\n",
        "\n",
        "    quantized_model.train()\n",
        "\n",
        "    # Set calibration_steps to a larger number if possible.\n",
        "    calibration_steps = 1000\n",
        "\n",
        "    # Ensure more unique calibration data\n",
        "    for i in range(calibration_steps):\n",
        "        calibration_data = torch.randn([1, 3, 640, 640], dtype=torch.float32)\n",
        "        quantized_model(calibration_data)\n",
        "\n",
        "    quantized_model.eval()\n",
        "    quantized_model.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        new_data = new_data.to(\"cuda\")\n",
        "        output = quantized_model(new_data)\n",
        "        print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIqMvIdnI-sw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}